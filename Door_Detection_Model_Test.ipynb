{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dysonspheres/FRI-ML-Door-Detection-Model/blob/main/Door_Detection_Model_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Information for this project\n",
        "\n",
        "* When creating strings for file paths, do not include the content folder. The google collab runtime is already inside of the content folder\n",
        "* Run blocks sequently to train model correctly"
      ],
      "metadata": {
        "id": "ruAOVZpzuJGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_RqoEE8YiYF"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "data_dir = './drive/MyDrive/image_dump'"
      ],
      "metadata": {
        "id": "blfjF8NDrud0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_data_dir = './data'\n",
        "\n",
        "# This code block creates the necessary file structure in the LOCAL google collab file system\n",
        "if not os.path.isdir(local_data_dir):\n",
        "  os.mkdir(local_data_dir)\n",
        "\n",
        "training_dir = os.path.join(local_data_dir, 'training')\n",
        "if not os.path.isdir(training_dir):\n",
        "  os.mkdir(training_dir)\n",
        "\n",
        "closed_training_dir = os.path.join(training_dir, 'closed/')\n",
        "if not os.path.isdir(closed_training_dir):\n",
        "  os.mkdir(closed_training_dir)\n",
        "\n",
        "open_training_dir = os.path.join(training_dir, 'open/')\n",
        "if not os.path.isdir(open_training_dir):\n",
        "  os.mkdir(open_training_dir)\n",
        "\n",
        "#create validation dir (for testing)\n",
        "validation_dir = os.path.join(local_data_dir,'validation')\n",
        "if not os.path.isdir(validation_dir):\n",
        "  os.mkdir(validation_dir)\n",
        "\n",
        "#create closed in validation\n",
        "closed_validation_dir = os.path.join(validation_dir, 'closed/')\n",
        "if not os.path.isdir(closed_validation_dir):\n",
        "  os.mkdir(closed_validation_dir)\n",
        "\n",
        "#create open in validation\n",
        "open_validation_dir = os.path.join(validation_dir,'open/')\n",
        "if not os.path.isdir(open_validation_dir):\n",
        "  os.mkdir(open_validation_dir)"
      ],
      "metadata": {
        "id": "_t_MDl97Yqpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import random\n",
        "\n",
        "# this code block sorts images from Casey's drive file (image_dump) into \n",
        "# the train and validation data sets with an 80-20 split\n",
        "# DO NOT LOAD THIS TWICE IN THE SAME RUNTIME, IT WILL RUIN THE DATASETS\n",
        "\n",
        "data_dir = './drive/MyDrive/image_dump'\n",
        "split = 0.8\n",
        "closed_img_dir = data_dir + '/closed/closed*'\n",
        "print(closed_img_dir) # debug, remove later\n",
        "open_img_dir = data_dir + '/open/open*'\n",
        "closed_imgs_size = len(glob.glob(closed_img_dir))\n",
        "open_imgs_size = len(glob.glob(open_img_dir))\n",
        "print(closed_imgs_size) # debug, remove later\n",
        "\n",
        "closed_list = glob.glob(closed_img_dir)\n",
        "open_list = glob.glob(open_img_dir)\n",
        "\n",
        "random.shuffle(closed_list)\n",
        "random.shuffle(open_list)\n",
        "\n",
        "closed_list = enumerate(closed_list)\n",
        "open_list = enumerate(open_list)\n",
        "\n",
        "for i, img in closed_list:\n",
        "  if i < (closed_imgs_size * split):\n",
        "    if not os.path.exists(closed_training_dir + os.path.basename(img)):\n",
        "      shutil.move(img, closed_training_dir)\n",
        "  else:\n",
        "    if not os.path.exists(closed_validation_dir + os.path.basename(img)):\n",
        "      shutil.move(img, closed_validation_dir)\n",
        "\n",
        "for i, img in open_list:\n",
        "  if i < (open_imgs_size * split):\n",
        "    if not os.path.exists(open_training_dir + os.path.basename(img)):\n",
        "      shutil.move(img, open_training_dir)\n",
        "  else:\n",
        "    if not os.path.exists(open_validation_dir + os.path.basename(img)):\n",
        "      shutil.move(img, open_validation_dir)"
      ],
      "metadata": {
        "id": "7iqLhElGpJ4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5779a374-79f9-409d-8ceb-9d0c0c68127c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./drive/MyDrive/image_dump/closed/closed*\n",
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAggtmEK_kPB",
        "outputId": "8fed0412-619d-4eab-f691-e3114f19e918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code blocks serves to show you a random sample of the pulled images. It has no other purpose for the actual model itself\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy import ndimage\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "\n",
        "samples_closed = [os.path.join(closed_training_dir,np.random.choice(os.listdir(closed_training_dir),1)[0]) for _ in range(8)]\n",
        "samples_open = [os.path.join(open_training_dir,np.random.choice(os.listdir(open_training_dir),1)[0]) for _ in range(8)]\n",
        "\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "fig, ax = plt.subplots(nrows,ncols,figsize = (10,10))\n",
        "ax = ax.flatten()\n",
        "\n",
        "for i in range(nrows*ncols):\n",
        "  if i < 8:\n",
        "    pic = plt.imread(samples_closed[i%8])\n",
        "    ax[i].imshow(pic)\n",
        "    ax[i].set_axis_off()\n",
        "  else:\n",
        "    pic = plt.imread(samples_open[i%8])\n",
        "    ax[i].imshow(pic)\n",
        "    ax[i].set_axis_off()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KzWR_g5IqPiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_dir = './data/training'\n",
        "test_dir = './data/validation'\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                       transforms.ToTensor(),                                \n",
        "                                       torchvision.transforms.Normalize(\n",
        "                                           mean=[0.485, 0.456, 0.406],\n",
        "                                           std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "                                       ])\n",
        "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      torchvision.transforms.Normalize(\n",
        "                                          mean=[0.485, 0.456, 0.406],\n",
        "                                          std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "                                      ])\n",
        "\n",
        "#datasets\n",
        "train_data = datasets.ImageFolder(train_dir,transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(test_dir,transform=test_transforms)\n",
        "\n",
        "#dataloader\n",
        "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=16)\n",
        "testloader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size=16)"
      ],
      "metadata": {
        "id": "cT_h6HqfrvJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile everything needed to train the model into one function\n",
        "\n",
        "def make_train_step(model, optimizer, loss_fn):\n",
        "  def train_step(x,y):\n",
        "    #make prediction\n",
        "    yhat = model(x)\n",
        "    #enter train mode\n",
        "    model.train()\n",
        "    #compute loss\n",
        "    loss = loss_fn(yhat,y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    #optimizer.cleargrads()\n",
        "\n",
        "    return loss\n",
        "  return train_step"
      ],
      "metadata": {
        "id": "zbjuXZYVuqwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for the model itself\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models import ResNet18_Weights\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "#freeze all params\n",
        "for params in model.parameters():\n",
        "  params.requires_grad_ = False\n",
        "\n",
        "#add a new final layer\n",
        "nr_filters = model.fc.in_features  #number of input features of last layer\n",
        "model.fc = nn.Linear(nr_filters, 1)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Qk6QCzKLvUlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c8ac06-7640-4fb7-fae2-37f5fbcbd0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 25.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for the loss, optimizer, and train step\n",
        "\n",
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "#loss\n",
        "loss_fn = BCEWithLogitsLoss() #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.Adam(model.fc.parameters()) \n",
        "\n",
        "#train step\n",
        "train_step = make_train_step(model, optimizer, loss_fn)"
      ],
      "metadata": {
        "id": "Ttt7hzKWvPmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ACTUAL MODEL TRAINING CODE WILL TAKE FOREVER (play chess while waiting)\n",
        "\n",
        "%%capture\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "\n",
        "n_epochs = 10\n",
        "early_stopping_tolerance = 3\n",
        "early_stopping_threshold = 0.03\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  epoch_loss = 0\n",
        "  for i ,data in tqdm(enumerate(trainloader), total = len(trainloader)): #iterate ove batches\n",
        "    x_batch , y_batch = data\n",
        "    x_batch = x_batch.to(device) #move to gpu\n",
        "    y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "    y_batch = y_batch.to(device) #move to gpu\n",
        "\n",
        "\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "    epoch_loss += loss/len(trainloader)\n",
        "    losses.append(loss)\n",
        "    \n",
        "  epoch_train_losses.append(epoch_loss)\n",
        "  print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "\n",
        "  #validation doesnt requires gradient\n",
        "  with torch.no_grad():\n",
        "    cum_loss = 0\n",
        "    for x_batch, y_batch in testloader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      #model to eval mode\n",
        "      model.eval()\n",
        "\n",
        "      yhat = model(x_batch)\n",
        "      val_loss = loss_fn(yhat,y_batch)\n",
        "      cum_loss += loss/len(testloader)\n",
        "      val_losses.append(val_loss.item())\n",
        "\n",
        "\n",
        "    epoch_test_losses.append(cum_loss)\n",
        "    print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))  \n",
        "    \n",
        "    best_loss = min(epoch_test_losses)\n",
        "    \n",
        "    #save best model\n",
        "    if cum_loss <= best_loss:\n",
        "      best_model_wts = model.state_dict()\n",
        "    \n",
        "    #early stopping\n",
        "    early_stopping_counter = 0\n",
        "    if cum_loss > best_loss:\n",
        "      early_stopping_counter +=1\n",
        "\n",
        "    if (early_stopping_counter == early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n",
        "      print(\"/nTerminating: early stopping\")\n",
        "      break #terminate training\n",
        "    \n",
        "#load best model\n",
        "model.load_state_dict(best_model_wts)\n"
      ],
      "metadata": {
        "id": "WfZd1kmivkHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import transforms\n",
        "\n",
        "torch.save(model.state_dict(), './drive/MyDrive/image_dump/model.pth')\n",
        "\n",
        "\n",
        "\n",
        "def inference(test_data):\n",
        "  idx = torch.randint(1, len(test_data), (1,))\n",
        "  sample = torch.unsqueeze(test_data[idx][0], dim=0).to(device)\n",
        "\n",
        "  if torch.sigmoid(model(sample)) < 0.5:\n",
        "    print(\"Prediction : Closed\")\n",
        "  else:\n",
        "    print(\"Prediction : Open\")\n",
        "\n",
        "  img = test_data[idx][0].permute(1, 2, 0)\n",
        "  rotated_img = ndimage.rotate(img, -90)\n",
        "\n",
        "  plt.imshow(rotated_img)\n",
        "\n",
        "inference(test_data)"
      ],
      "metadata": {
        "id": "7SxgFj8Pvt8i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}